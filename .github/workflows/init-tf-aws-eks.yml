# Name of the workflow
name: Init AWS EKS CD

# Define environment variables
env:
  # AWS Access Key
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
  # AWS Secret Key
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_KEY }}

# Define the events that trigger the workflow
on:
  # Trigger the workflow manually from the Actions tab
  workflow_dispatch:
    # It requires several inputs provided by the user
    inputs:
      # These inputs are details for the EKS environment, deployment parameters, and tags
      cluster_name:
        description: 'Enter a name for the cluster'
        required: true   
      vpc_id:
        description: 'Enter the VPC id i.e. vpc-00000000000000001'
        required: true        
      service_cidr:
        description: 'Enter the internal EKS service cidr to use within the cluster i.e. 10.200.0.0/16'
        required: true          
      vpc_subnet_ids:
        description: 'Enter 3 private subnets ids, comma seperated i.e. "subnet-00000000000000001,subnet-00000000000000002,subnet-00000000000000003"'
        required: true   
      vpc_public_subnet_id:
        description: 'Enter the public subnet id to deploy the jumpbox in i.e. "subnet-00000000000000001"'
        required: true         
      capacity_type:
        description: 'Select the deployment type'
        required: true
        type: choice
        options:
        - 'SPOT'
        - 'ON_DEMAND'
      client_tag:
        description: 'Enter the value to be used in the client tag (all lowercase) i.e. "client1" or "shared"'
        required: true    
      product_tag:
        description: 'Enter the value to be used in the product tag (all lowercase) i.e. "client1" or "shared"'
        required: true          
      environment_tag:
        description: 'Enter the value to be used in the env tag (all lowercase) i.e. "prod" or "dev"'
        required: true            

# Define permissions for this workflow
permissions:
  # Only read access to the contents is needed
  contents: read

# Define jobs in this workflow  
jobs:
  # Define a job for Terraform actions
  generate-main-tf:
    # Name of the job  
    name: 'Terraform EKS main.tf Generation'
    # Define the type of runner that the job will run on
    runs-on: [self-hosted, devops]
    # Define the environment in which the job will run
    environment: production

    # Define steps for this job
    steps:
      # Checkout the repository to the GitHub Actions runner 
      - name: Checkout
        uses: actions/checkout@v3    

      # If main.tf does not exist, we generate a new one using user input
      - name: Check if main.tf exists
        run: |
          if [ -f "main.tf" ]; then
            echo "main.tf already exists, skipping file generation."
            echo "file_exists=true" >> $GITHUB_OUTPUT
          else
            echo "main.tf does not exist, continuing with file generation."
            echo "file_exists=false" >> $GITHUB_OUTPUT
          fi
        id: check-main-tf
      
      - name: Generate main.tf with user input
        if: steps.check-main-tf.outputs.file_exists == 'false'
        run: |
          DNS_IP=$(echo "${{ github.event.inputs.service_cidr }}" | python3 -c 'import ipaddress, sys; cidr = ipaddress.ip_network(sys.stdin.read().strip()); print(list(cidr.hosts())[9])')
          cat > main.tf << EOF
          # Define the required version of Terraform and the required AWS provider version
          terraform {
            required_version = ">= 1.5.0"

            backend "s3" {}
            
            required_providers {
              aws = {
                source  = "hashicorp/aws"
                version = "~> 5.0"
              }
              
            }
            
          }

          # Configure the AWS Provider
          provider "aws" {
            region = "eu-west-2"
          }

          # Define the module source and its location                    
          module "eks_cluster" {
            source = "./modules/aws-eks-cluster"          

            name                 = "<cluster_name>"
            eks_version          = "1.29"
            service_ipv4_cidr    = "<service_cidr>"
            core_dns_ip          = "${DNS_IP}"
            subnet_ids           = ["<vpc_subnet_id_1>", "<vpc_subnet_id_2>", "<vpc_subnet_id_3>"]
            public_subnet_id     = "<vpc_public_subnet_id>"
            vpc_id               = "<vpc_id>"          
            aws_access           = "<aws_access>"
            aws_secret           = "<aws_secret>"            

            addons = [
              {
                name    = "kube-proxy"
              },
              {
                name    = "vpc-cni"
              },
              {
                name    = "coredns"
              },
              {
                name    = "aws-ebs-csi-driver"
              }
            ]          
            
            # Configuration for the first node pool
            node_pools = [
              {
                capacity_type = "<capacity_type>"
                node_group_name = "<cluster_name>-ng-<capacity_type_lowercase>"
                subnet_ids      = ["<vpc_subnet_id_1>", "<vpc_subnet_id_2>", "<vpc_subnet_id_3>"]
                tags = {
                    "environment"                                         = "<environment_tag>"
                    "client"                                              = "<client_tag>"
                    "product_tag"                                         = "<product_tag>"
                }
                desired_size = 3
                max_size     = 3
                min_size     = 1
                instance_types = ["m5a.2xlarge"]
            
                labels = {
                    "lifecycle" = "<capacity_type_lowercase>"
                    "managedby" = "terraform"
                }
                taint = []
              },
            ]          
            
            # Additional tagging for the EKS cluster.
            client_tag           = "<client_tag>"
            product_tag          = "<product_tag>"
            environment_tag         = "<environment_tag>"
          }          

          # Outputs from this configuration that will be displayed when Terraform is applied.
          output "eks_cluster_name" {
            value = module.eks_cluster.eks_cluster_name
          }          

          output "eks_cluster_endpoint" {
            value = module.eks_cluster.eks_cluster_endpoint
          }          

          output "eks_cluster_arn" {
            value = module.eks_cluster.eks_cluster_arn
          }          

          output "eks_cluster_version" {
            value = module.eks_cluster.eks_cluster_version
          }          

          output "eks_worker_nodes_security_group_id" {
            value = module.eks_cluster.eks_worker_nodes_security_group_id
          }          

          output "eks_cluster_security_group_id" {
            value = module.eks_cluster.eks_cluster_security_group_id
          }          

          output "eks_cluster_kms_key_arn" {
            value = module.eks_cluster.eks_cluster_kms_key_arn
          }
          
          output "encrypted_password_data" {
            value = module.eks_cluster.encrypted_password_data
            sensitive = true
          }
          
          EOF

          # Replace the placeholders with the actual input values
          sed -i "s|<cluster_name>|${{ github.event.inputs.cluster_name }}|g" main.tf
          sed -i "s|<vpc_id>|${{ github.event.inputs.vpc_id }}|g" main.tf
          sed -i "s|<service_cidr>|${{ github.event.inputs.service_cidr }}|g" main.tf

          IFS=',' read -ra VPC_SUBNET_IDS <<< "${{ github.event.inputs.vpc_subnet_ids }}"
          sed -i "s|<vpc_subnet_id_1>|${VPC_SUBNET_IDS[0]}|g" main.tf
          sed -i "s|<vpc_subnet_id_2>|${VPC_SUBNET_IDS[1]}|g" main.tf
          sed -i "s|<vpc_subnet_id_3>|${VPC_SUBNET_IDS[2]}|g" main.tf

          sed -i "s|<vpc_public_subnet_id>|${{ github.event.inputs.vpc_public_subnet_id }}|g" main.tf
          
          sed -i "s|<capacity_type>|${{ github.event.inputs.capacity_type }}|g" main.tf
          CAPACITY_LOWER=$(echo "${{ github.event.inputs.capacity_type }}" | tr 'A-Z' 'a-z')
          sed -i "s|<capacity_type_lowercase>|${CAPACITY_LOWER}|g" main.tf
          
          sed -i "s|<client_tag>|${{ github.event.inputs.client_tag }}|g" main.tf
          sed -i "s|<product_tag>|${{ github.event.inputs.product_tag }}|g" main.tf
          sed -i "s|<environment_tag>|${{ github.event.inputs.environment_tag }}|g" main.tf
          sed -i "s|<aws_access>|${{ env.AWS_ACCESS_KEY_ID }}|g" main.tf
          sed -i "s|<aws_secret>|${{ env.AWS_SECRET_ACCESS_KEY }}|g" main.tf          
           
          # Print the generated main.tf
          cat main.tf

      # If a new main.tf file was created, commit and push it to the repository
      - name: Commit and push main.tf
        if: steps.check-main-tf.outputs.file_exists == 'false'
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "github-actions@users.noreply.github.com"
          git add main.tf
          git commit -m "Add generated main.tf"
          git push
          
