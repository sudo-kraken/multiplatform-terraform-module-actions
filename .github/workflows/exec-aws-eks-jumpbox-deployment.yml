# Name of the workflow
name: 'Execute AWS EKS Jumpbox CD'

# Define environment variables
env:
  # AWS Access Key
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
  # AWS Secret Key
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_KEY }}

# Define the events that trigger the workflow
on:
  # Trigger the workflow manually from the Actions tab
  workflow_dispatch:
    # It requires several inputs provided by the user
    inputs:

# Define permissions for this workflow
permissions:
  # Only read access to the contents is needed
  contents: read

# Define jobs in this workflow
jobs:
  # Define a job for Terraform actions
  terraform:
    # Name of the job
    name: 'Terraform AWS EKS Jumpbox Pipeline'
    # Define the type of runner that the job will run on
    runs-on: [self-hosted, devops]
    # Define the environment in which the job will run
    environment: production

    # Set default options for steps in this job
    defaults:
      run:
        # Use the bash shell
        shell: bash

    # Define steps for this job
    steps:
    # Checkout the repository to the GitHub Actions runner 
    - name: Checkout
      uses: actions/checkout@v3
                
    # Setup Node.js environment with a specific version
    - name: Node Setup
      uses: actions/setup-node@v3
      with:
        node-version: '16'

    # Set up AWS CLI
    - name: Set up AWS CLI
      uses: aws-actions/configure-aws-credentials@v3
      with:
        aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
        aws-region: eu-west-2     

    # Get Cluster Name
    - name: Get Cluster Name
      run: |
        echo "cluster_name=$(grep 'name\s*=' ./main.tf | awk -F"=" 'NR==1 {print $2}' | xargs)" >> $GITHUB_ENV         
    
    # Setup Terraform CLI on the runner
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v2
    
    # Initialise your Terraform working directory 
    - name: Terraform Init S3    
      run: |
        terraform init -backend-config="bucket=global-gh-tf-state" \
                       -backend-config="key=terraform-state-files/${{ env.cluster_name }}-jumpbox-tf-state/terraform.tfstate" \
                       -backend-config="encrypt=true" \
                       -backend-config="kms_key_id=arn:aws:kms:eu-west-2:accountnumberhere:key/key-id-here" \
                       -backend-config="region=eu-west-2" \
                       -backend-config="access_key=${{ env.AWS_ACCESS_KEY_ID }}" \
                       -backend-config="secret_key=${{ env.AWS_SECRET_ACCESS_KEY }}"

    # Generate an execution plan for Terraform
    - name: Terraform Plan
      run: terraform plan

      # Apply the changes required to reach the desired state of the configuration
    - name: Terraform Apply
      id: apply    
      continue-on-error: true
      run: terraform apply -auto-approve

    # Check if Terraform Apply failed and set an output variable
    - name: Check Terraform Apply Outcome
      id: check
      run: echo "apply_failed=${{ steps.apply.outcome == 'failure' }}" >> $GITHUB_ENV      
      
    # Destroy if Terraform Apply failed
    - name: Terraform Destroy
      if: env.apply_failed == 'true'
      id: failure-handler
      run: |
        echo "Terraform Apply failed. Initiating cleanup..."
        terraform destroy -auto-approve
        echo "cleanup_done=true" >> $GITHUB_ENV     
      
    # Upload the main.tf to s3
    - name: S3 Upload
      if: env.apply_failed != 'true'
      run:  |
        aws s3 cp main.tf s3://global-gh-tf-state/terraform-state-files/${{ env.cluster_name }}-jumpbox-tf-state/main.tf
        if aws s3api head-object --bucket "global-gh-tf-state" --key "terraform-state-files/${{ env.cluster_name }}-jumpbox-tf-state/private_key.pem" 2>/dev/null; then
          echo "File already exists in S3. Not overwriting."
        else 
          aws s3 cp ${{ github.workspace }}/modules/aws-ec2-jumpbox/private_key.pem s3://global-gh-tf-state/terraform-state-files/${{ env.cluster_name }}-jumpbox-tf-state/private_key.pem 
        fi